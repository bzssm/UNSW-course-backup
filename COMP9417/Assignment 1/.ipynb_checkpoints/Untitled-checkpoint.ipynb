{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Nearest Neighbour Results                                                 \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |  Baseline  |       10%        |       25%        |       50%        |       75%        |       100%      \n",
      "anneal          |     23.83% | 20.31% (0.94%) * | 18.00% (1.33%) * | 11.12% (0.66%) * |  9.11% (0.37%) * |  7.44% (0.44%) *\n",
      "audiology       |     74.77% | 60.17% (2.17%) * | 42.00% (2.56%) * | 31.85% (2.13%) * | 29.62% (1.78%) * | 26.47% (1.81%) *\n",
      "autos           |     67.35% | 64.50% (1.50%) * | 61.40% (2.21%) * | 65.96% (2.02%)   | 52.92% (2.39%) * | 57.37% (0.95%) *\n",
      "credit-a        |     44.49% | 39.98% (1.05%) * | 41.35% (0.99%) * | 32.04% (1.50%) * | 34.63% (0.79%) * | 34.71% (0.73%) *\n",
      "hypothyroid     |      7.71% |  8.27% (0.52%) * |  7.33% (0.18%) * |  4.74% (0.14%) * |  5.01% (0.13%) * |  4.79% (0.10%) *\n",
      "letter          |     96.26% | 16.86% (0.35%) * |  9.61% (0.20%) * |  6.05% (0.08%) * |  4.71% (0.06%) * |  3.93% (0.07%) *\n",
      "microarray      |     50.20% | 59.47% (2.55%) * | 49.58% (2.36%)   | 42.45% (0.83%) * | 50.71% (0.95%)   | 50.88% (0.60%)  \n",
      "vote            |     38.63% |  6.45% (1.01%) * | 10.42% (1.16%) * |  8.26% (0.55%) * |  7.12% (0.19%) * |  7.91% (0.39%) *\n",
      "\n",
      "                                                   Decision Tree Results                                                   \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |  Baseline  |       10%        |       25%        |       50%        |       75%        |       100%      \n",
      "anneal          |     23.83% |  9.32% (1.94%) * |  3.84% (0.93%) * |  1.45% (0.38%) * |  1.32% (0.28%) * |  0.76% (0.24%) *\n",
      "audiology       |     74.77% | 60.83% (5.23%) * | 45.63% (4.41%) * | 28.61% (2.21%) * | 22.15% (2.63%) * | 22.70% (2.19%) *\n",
      "autos           |     67.35% | 68.50% (5.02%)   | 46.43% (6.50%) * | 34.96% (2.81%) * | 29.32% (3.91%) * | 21.55% (2.95%) *\n",
      "credit-a        |     44.49% | 20.10% (2.95%) * | 13.25% (1.12%) * | 19.99% (1.79%) * | 19.66% (1.26%) * | 19.43% (1.31%) *\n",
      "hypothyroid     |      7.71% |  2.84% (0.48%) * |  1.62% (0.20%) * |  0.66% (0.06%) * |  0.75% (0.10%) * |  0.60% (0.04%) *\n",
      "letter          |     96.26% | 28.99% (0.54%) * | 21.45% (0.43%) * | 16.46% (0.34%) * | 13.38% (0.12%) * | 11.85% (0.15%) *\n",
      "microarray      |     50.20% | 52.30% (4.03%)   | 51.55% (2.84%)   | 50.95% (1.99%)   | 46.67% (2.02%) * | 49.09% (1.20%)  \n",
      "vote            |     38.63% | 14.55% (3.03%) * |  5.35% (1.51%) * |  6.86% (1.27%) * |  3.23% (0.59%) * |  5.75% (0.50%) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code for question 1\n",
    "\n",
    "import arff\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "seeds = [2, 3, 5, 7, 11, 13, 17, 23, 29, 31, 37]\n",
    "score_list = []\n",
    "\n",
    "for fname in [\"anneal.arff\", \"audiology.arff\", \"autos.arff\", \"credit-a.arff\", \\\n",
    "              \"hypothyroid.arff\", \"letter.arff\", \"microarray.arff\", \"vote.arff\"]:\n",
    "    dataset = arff.load(open(fname), 'r')\n",
    "    data = np.array(dataset['data'])\n",
    "\n",
    "    X = np.array(data)[:, :-1]\n",
    "    Y = np.array(data)[:, -1]\n",
    "\n",
    "    # turn unknown/none/? into a separate value\n",
    "    for i, j in product(range(len(data)), range(len(data[0]) - 1)):\n",
    "        if X[i, j] is None:\n",
    "            X[i, j] = len(dataset['attributes'][j][1])\n",
    "\n",
    "    # a hack to turn negative categories positive for autos.arff\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i] < 0:\n",
    "            Y[i] += 7\n",
    "\n",
    "    # identify and extract categorical/non-categorical features\n",
    "    categorical, non_categorical = [], []\n",
    "    for i in range(len(dataset['attributes']) - 1):\n",
    "        if isinstance(dataset['attributes'][i][1], str):\n",
    "            non_categorical.append(X[:, i])\n",
    "        else:\n",
    "            categorical.append(X[:, i])\n",
    "\n",
    "    categorical = np.array(categorical).T\n",
    "    non_categorical = np.array(non_categorical).T\n",
    "\n",
    "    if categorical.shape[0] == 0:\n",
    "        transformed_X = non_categorical\n",
    "    else:\n",
    "        # encode categorical features\n",
    "        encoder = OneHotEncoder(n_values = 'auto',\n",
    "                                categorical_features = 'all',\n",
    "                                dtype = np.int32,\n",
    "                                sparse = False,\n",
    "                                handle_unknown = 'error')\n",
    "        encoder.fit(categorical)\n",
    "        categorical = encoder.transform(categorical)\n",
    "        if non_categorical.shape[0] == 0:\n",
    "            transformed_X = categorical\n",
    "        else:\n",
    "            transformed_X = np.concatenate((categorical, non_categorical), axis = 1)\n",
    "\n",
    "    # concatenate the feature array and the labels for resampling purpose\n",
    "    Y = np.array([Y], dtype = np.int)\n",
    "    input_data = np.concatenate((transformed_X, Y.T), axis = 1)\n",
    "\n",
    "    # build the models\n",
    "    models = [DummyClassifier(strategy = 'most_frequent')] \\\n",
    "              + [KNeighborsClassifier(n_neighbors = 1, algorithm = \"brute\")] * 5 \\\n",
    "              + [DecisionTreeClassifier()] * 5\n",
    "\n",
    "    # resample and run cross validation\n",
    "    portion = [1.0, 0.1, 0.25, 0.5, 0.75, 1.0, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "    sample, scores = [None] * 11, [None] * 11\n",
    "    for i in range(11):\n",
    "        sample[i] = resample(input_data,\n",
    "                             replace = False,\n",
    "                             n_samples = int(portion[i] * input_data.shape[0]),\n",
    "                             random_state = seeds[i])\n",
    "        score = [None] * 10\n",
    "        for j in range(10):\n",
    "            score[j] = np.mean(cross_val_score(models[i],\n",
    "                                               sample[i][:, :-1],\n",
    "                                               sample[i][:, -1].astype(np.int),\n",
    "                                               scoring = 'accuracy',\n",
    "                                               cv = KFold(10, True, seeds[j])))\n",
    "        scores[i] = score\n",
    "\n",
    "    score_list.append((fname[:-5], 1 - np.array(scores)))\n",
    "\n",
    "# print the results\n",
    "header = [\"{:^123}\".format(\"Nearest Neighbour Results\") + '\\n' + '-' * 123  + '\\n' + \\\n",
    "          \"{:^15} | {:^10} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16}\" \\\n",
    "          .format(\"Dataset\", \"Baseline\", \"10%\", \"25%\", \"50%\", \"75%\", \"100%\"),\n",
    "          \"{:^123}\".format(\"Decision Tree Results\") + '\\n' + '-' * 123  + '\\n' + \\\n",
    "          \"{:^15} | {:^10} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16}\" \\\n",
    "          .format(\"Dataset\", \"Baseline\", \"10%\", \"25%\", \"50%\", \"75%\", \"100%\")]\n",
    "offset = [1, 6]\n",
    "\n",
    "for k in range(2):\n",
    "    print(header[k])\n",
    "    for i in range(8):\n",
    "        scores = score_list[i][1]\n",
    "        p_value = [None] * 5\n",
    "        for j in range(5):\n",
    "            _, p_value[j] = ttest_ind(scores[0], scores[j + offset[k]], equal_var = False)\n",
    "\n",
    "        print(\"{:<15} | {:>10.2%}\".format(score_list[i][0], np.mean(scores[0])), end = '')\n",
    "        for j in range(5):\n",
    "            print(\" | {:>6.2%} ({:>5.2%}) {}\" .format(np.mean(scores[j + offset[k]]),\n",
    "                                                      np.std(scores[j + offset[k]]),\n",
    "                                                      '*' if p_value[j] < 0.05 else ' '), end = '')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.557774316174719, 40.558189843069975, 49.792104941866995, 51.932469021123261, 52.013933334915599]\n",
      "[36.161000883402004, 53.095090487293405, 60.334961248573919, 66.159896624432804, 67.335859969459477]\n",
      "23.6029803015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "errs=[]\n",
    "mean = 0\n",
    "for k in range(2): \n",
    "    errs=[np.mean([np.mean(score_list[i][1][j+offset[k]]) for i in range(8)]) for j in range(5)]\n",
    "    err0 = np.mean([score_list[i][1][0] for i in range(8)])\n",
    "    print([(err0-e)/err0*100 for e in errs])\n",
    "\n",
    "\n",
    "errs = np.mean([(np.mean(score_list[i][1][0])-np.mean(score_list[i][1][1]))/np.mean(score_list[i][1][0])*100 for i in range(8)])\n",
    "print(errs)\n",
    "errs = np.mean([(np.mean(score_list[i][1][0])-np.mean(score_list[i][1][5]))/np.mean(score_list[i][1][0])*100 for i in range(8)])\n",
    "print(errs)\n",
    "errs = np.mean([(np.mean(score_list[i][1][0])-np.mean(score_list[i][1][6]))/np.mean(score_list[i][1][0])*100 for i in range(8)])\n",
    "print(errs)\n",
    "errs = np.mean([(np.mean(score_list[i][1][0])-np.mean(score_list[i][1][10]))/np.mean(score_list[i][1][0])*100 for i in range(8)])\n",
    "print(errs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Decision Tree Results                                                   \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |     Default      |        0%        |       20%        |       50%        |       80%        |\n",
      "balance-scale   |  36.70% ( 2)     |  76.06% ( 2)     |  71.28% (12)     |  65.43% (27)     |  18.09% (27)     |\n",
      "\n",
      "primary-tumor   |  25.49% ( 2)     |  37.25% (12)     |  42.16% (12)     |  43.14% (12)     |  26.47% ( 7)     |\n",
      "\n",
      "glass           |  44.62% ( 2)     |  69.23% ( 7)     |  66.15% (22)     |  35.38% (17)     |  29.23% (17)     |\n",
      "\n",
      "heart-h         |  35.96% ( 2)     |  67.42% ( 7)     |  78.65% (22)     |  56.18% (17)     |  20.22% (27)     |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code for question 2\n",
    "\n",
    "import arff, numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# fixed random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def label_enc(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le\n",
    "\n",
    "def features_encoders(features,categorical_features='all'):\n",
    "    n_samples, n_features = features.shape\n",
    "    label_encoders = [preprocessing.LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        label_encoders[i].fit(feature_i)\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "        \n",
    "    enc = preprocessing.OneHotEncoder(categorical_features=categorical_features)\n",
    "    return enc.fit(X_int),label_encoders\n",
    "\n",
    "def feature_transform(features,label_encoders, one_hot_encoder):\n",
    "    \n",
    "    n_samples, n_features = features.shape\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "\n",
    "    return one_hot_encoder.transform(X_int).toarray()\n",
    "\n",
    "warnings.warn = warn\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    dataset = arff.load(open(path, 'r'))\n",
    "    data = np.array(dataset['data'])\n",
    "    data = pd.DataFrame(data)\n",
    "    data = DataFrameImputer().fit_transform(data).values\n",
    "    attr = dataset['attributes']\n",
    "\n",
    "    # mask categorical features\n",
    "    masks = []\n",
    "    for i in range(len(attr)-1):\n",
    "        if attr[i][1] != 'REAL':\n",
    "            masks.append(i)\n",
    "    return data, masks\n",
    "\n",
    "def preprocess(data,masks, noise_ratio):\n",
    "    # split data\n",
    "    train_data, test_data = train_test_split(data,test_size=0.3,random_state=0)\n",
    "\n",
    "    # test data\n",
    "    test_features = test_data[:,0:test_data.shape[1]-1]\n",
    "    test_labels = test_data[:,test_data.shape[1]-1]\n",
    "\n",
    "    # training data\n",
    "    features = train_data[:,0:train_data.shape[1]-1]\n",
    "    labels = train_data[:,train_data.shape[1]-1]\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    # categorical features need to be encoded\n",
    "    if len(masks):\n",
    "        one_hot_enc, label_encs = features_encoders(data[:,0:data.shape[1]-1],masks)\n",
    "        test_features = feature_transform(test_features,label_encs,one_hot_enc)\n",
    "        features = feature_transform(features,label_encs,one_hot_enc)\n",
    "\n",
    "    le = label_enc(data[:,data.shape[1]-1])\n",
    "    labels = le.transform(train_data[:,train_data.shape[1]-1])\n",
    "    test_labels = le.transform(test_data[:,test_data.shape[1]-1])\n",
    "    \n",
    "    # add noise\n",
    "    np.random.seed(1234)\n",
    "    noise = np.random.randint(len(classes)-1, size=int(len(labels)*noise_ratio))+1\n",
    "    \n",
    "    noise = np.concatenate((noise,np.zeros(len(labels) - len(noise),dtype=np.int)))\n",
    "    labels = (labels + noise) % len(classes)\n",
    "\n",
    "    return features,labels,test_features,test_labels\n",
    "\n",
    "# load data\n",
    "paths = ['balance-scale','primary-tumor',\n",
    "         'glass','heart-h']\n",
    "noise = [0,0.2,0.5,0.8]\n",
    "\n",
    "scores = []\n",
    "params = []\n",
    "\n",
    "for path in paths:\n",
    "    score = []\n",
    "    param = []\n",
    "    path += '.arff'\n",
    "    data, masks = load_data(path)\n",
    "    \n",
    "    # training on data with %50 noise and default parameters\n",
    "    features, labels, test_features, test_labels = preprocess(data, masks, 0.5)\n",
    "    tree = DecisionTreeClassifier(random_state=0,min_samples_leaf=2, min_impurity_decrease=0)\n",
    "    tree.fit(features, labels)\n",
    "    tree_preds = tree.predict(test_features)\n",
    "    tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "    score.append(tree_performance)\n",
    "    param.append(tree.get_params()['min_samples_leaf'])\n",
    "    \n",
    "    # training on data with noise %0, %20, %50, %80\n",
    "    for noise_ratio in noise:\n",
    "        features, labels, test_features, test_labels = preprocess(data, masks, noise_ratio)\n",
    "        param_grid = {'min_samples_leaf': np.arange(2,30,5)}\n",
    "\n",
    "        grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid,cv=10,return_train_score=True)\n",
    "        grid_tree.fit(features, labels)\n",
    "\n",
    "        estimator = grid_tree.best_estimator_\n",
    "        tree_preds = grid_tree.predict(test_features)\n",
    "        tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "        score.append(tree_performance)\n",
    "        param.append(estimator.get_params()['min_samples_leaf'])\n",
    "\n",
    "    scores.append(score)\n",
    "    params.append(param)\n",
    "\n",
    "# print the results\n",
    "header = \"{:^123}\".format(\"Decision Tree Results\") + '\\n' + '-' * 123  + '\\n' + \\\n",
    "\"{:^15} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16} |\".format(\"Dataset\", \"Default\", \"0%\", \"20%\", \"50%\", \"80%\")\n",
    "\n",
    "\n",
    "# print result table\n",
    "print(header)\n",
    "for i in range(len(scores)):\n",
    "    #scores = score_list[i][1]\n",
    "    print(\"{:<16}\".format(paths[i]),end=\"\")\n",
    "    for j in range(len(params[i])):\n",
    "        print(\"|  {:>6.2%} ({:>2})     \" .format(scores[i][j],params[i][j]),end=\"\")\n",
    "    print('|\\n')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes:  [['median_house_value' 'REAL']\n",
      " ['median_income' 'REAL']\n",
      " ['housing_median_age' 'REAL']\n",
      " ['total_rooms' 'REAL']\n",
      " ['total_bedrooms' 'REAL']\n",
      " ['population' 'REAL']\n",
      " ['households' 'REAL']\n",
      " ['latitude' 'REAL']\n",
      " ['longitude' 'REAL']]\n",
      "Intercept:\n",
      "-3.59e+06\n",
      "Coefficients:\n",
      "4.02e+04 1.16e+03 -8.18e+00 1.13e+02 -3.85e+01 4.83e+01 -4.26e+04 -4.28e+04 \n",
      "RMSE:\n",
      "7.13e+04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code for question 3\n",
    "\n",
    "import arff,numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#--------------Show the attributes--------------\n",
    "\n",
    "dataset = arff.load(open('houses.arff',\"r\",encoding = \"ISO-8859-1\")) \n",
    "attributes = np.array(dataset['attributes'])\n",
    "print('attributes: ',attributes)\n",
    "\n",
    "#--------------linear regression--------------\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "data = np.array(dataset['data'])\n",
    "houses_X = data[:,1:] #X vector\n",
    "houses_Y = data[:,0] #Y vector\n",
    "\n",
    "regr.fit(houses_X, houses_Y)\n",
    "intercept = regr.intercept_\n",
    "\n",
    "print('Intercept:\\n%.2e' % intercept,end='\\n')\n",
    "print('Coefficients:')\n",
    "for coef in regr.coef_:\n",
    "    print('%.2e' % coef,end=\" \")\n",
    "\n",
    "file = open('q3.out','a')\n",
    "file.write('Intercept:\\n%.2e\\n' % intercept)\n",
    "file.write('Coefficients:\\n')\n",
    "for coef in regr.coef_:\n",
    "    file.write('%.2e' % coef)\n",
    "    file.write(' ')\n",
    "file.write('\\n')\n",
    "\n",
    "#--------------10-fold cross validation--------------\n",
    "\n",
    "predicted = cross_val_predict(regr, houses_X, houses_Y, cv=10)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(houses_Y, predicted))\n",
    "\n",
    "print ('\\nRMSE:\\n%.2e\\n' % RMSE)\n",
    "file.write('RMSE:\\n%.2e\\n' % RMSE)\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test accuracy using all features:  0.979324055666 0.805263157895\n",
      "Train/test accuracy for top 10K features 0.971769383698 0.805263157895\n",
      "Train/test accuracy for top 1K features 0.953876739563 0.694298245614\n",
      "Train/test accuracy for top 100 features 0.718787276342 0.463157894737\n",
      "Train/test accuracy for top 10 features 0.409244532803 0.189035087719\n"
     ]
    }
   ],
   "source": [
    "# code for question 4\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "df_trte = pd.read_csv('snippets_all.csv')\n",
    "df_tr = pd.read_csv('snippets_train.csv')\n",
    "df_te = pd.read_csv('snippets_test.csv')\n",
    "\n",
    "# set up the vocabulary (the global set of \"words\" or tokens) for training and test datasets\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df_trte.snippet)\n",
    "\n",
    "# apply this vocabulary to transform the text snippets to vectors of word counts\n",
    "X_train = vectorizer.transform(df_tr.snippet)\n",
    "X_test = vectorizer.transform(df_te.snippet)\n",
    "y_train = df_tr.section\n",
    "y_test = df_te.section\n",
    "\n",
    "# Debugging\n",
    "# print(\"X train: \", X_train.shape)\n",
    "# print(\"X test: \", X_test.shape)\n",
    "# print(\"Y train: \", y_train.shape)\n",
    "# print(\"Y test: \", y_test.shape)\n",
    "\n",
    "# learn a Naive Bayes classifier on the training set\n",
    "clf = MultinomialNB(alpha=0.5)\n",
    "MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf.fit(X_train, y_train)\n",
    "pred_train = clf.predict(X_train)\n",
    "score_train = metrics.accuracy_score(y_train, pred_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "score_test = metrics.accuracy_score(y_test, pred_test)\n",
    "print(\"Train/test accuracy using all features: \", score_train, score_test)\n",
    "\n",
    "\n",
    "# Use Chi^2 to select top 10000 features\n",
    "ch2_10000 = SelectKBest(chi2, k=10000)\n",
    "ch2_10000.fit(X_train, y_train)\n",
    "# Project training data onto top 10000 selected features\n",
    "X_train_kbest_10000 = ch2_10000.transform(X_train)\n",
    "# Train NB Classifier using top 10 selected features\n",
    "clf_kbest_10000 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_10000.fit(X_train_kbest_10000,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_10000 = clf_kbest_10000.predict(X_train_kbest_10000)\n",
    "score_train_kbest_10000 = metrics.accuracy_score(y_train,pred_train_kbest_10000)\n",
    "# Project test data onto top 10000 selected features\n",
    "X_test_kbest_10000 = ch2_10000.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_10000 = clf_kbest_10000.predict(X_test_kbest_10000)\n",
    "score_test_kbest_10000 = metrics.accuracy_score(y_test,pred_test_kbest_10000)\n",
    "print(\"Train/test accuracy for top 10K features\", score_train_kbest_10000, score_test_kbest_10000)\n",
    "\n",
    "# Use Chi^2 to select top 1000 features\n",
    "ch2_1000 = SelectKBest(chi2, k=1000)\n",
    "ch2_1000.fit(X_train, y_train)\n",
    "# Project training data onto top 1000 selected features\n",
    "X_train_kbest_1000 = ch2_1000.transform(X_train)\n",
    "# Train NB Classifier using top 1000 selected features\n",
    "clf_kbest_1000 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_1000.fit(X_train_kbest_1000,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_1000 = clf_kbest_1000.predict(X_train_kbest_1000)\n",
    "score_train_kbest_1000 = metrics.accuracy_score(y_train,pred_train_kbest_1000)\n",
    "# Project test data onto top 1000 selected features\n",
    "X_test_kbest_1000 = ch2_1000.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_1000 = clf_kbest_1000.predict(X_test_kbest_1000)\n",
    "score_test_kbest_1000 = metrics.accuracy_score(y_test,pred_test_kbest_1000)\n",
    "print(\"Train/test accuracy for top 1K features\", score_train_kbest_1000, score_test_kbest_1000)\n",
    "\n",
    "# Use Chi^2 to select top 100 features\n",
    "ch2_100 = SelectKBest(chi2, k=100)\n",
    "ch2_100.fit(X_train, y_train)\n",
    "# Project training data onto top 100 selected features\n",
    "X_train_kbest_100 = ch2_100.transform(X_train)\n",
    "# Train NB Classifier using top 100 selected features\n",
    "clf_kbest_100 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_100.fit(X_train_kbest_100,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_100 = clf_kbest_100.predict(X_train_kbest_100)\n",
    "score_train_kbest_100 = metrics.accuracy_score(y_train,pred_train_kbest_100)\n",
    "# Project test data onto top 100 selected features\n",
    "X_test_kbest_100 = ch2_100.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_100 = clf_kbest_100.predict(X_test_kbest_100)\n",
    "score_test_kbest_100 = metrics.accuracy_score(y_test,pred_test_kbest_100)\n",
    "print(\"Train/test accuracy for top 100 features\", score_train_kbest_100, score_test_kbest_100)\n",
    "\n",
    "# Use Chi^2 to select top 10 features\n",
    "ch2_10 = SelectKBest(chi2, k=10)\n",
    "ch2_10.fit(X_train, y_train)\n",
    "# Project training data onto top 10 selected features\n",
    "X_train_kbest_10 = ch2_10.transform(X_train)\n",
    "# Train NB Classifier using top 10 selected features\n",
    "clf_kbest_10 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_10.fit(X_train_kbest_10,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_10 = clf_kbest_10.predict(X_train_kbest_10)\n",
    "score_train_kbest_10 = metrics.accuracy_score(y_train,pred_train_kbest_10)\n",
    "# Project test data onto top 10 selected features\n",
    "X_test_kbest_10 = ch2_10.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_10 = clf_kbest_10.predict(X_test_kbest_10)\n",
    "score_test_kbest_10 = metrics.accuracy_score(y_test,pred_test_kbest_10)\n",
    "print(\"Train/test accuracy for top 10 features\", score_train_kbest_10, score_test_kbest_10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
