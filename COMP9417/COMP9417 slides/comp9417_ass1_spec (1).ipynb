{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMP9417 18s1  Assignment 1: Applying Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "_Last revision: Sat Mar 24 14:04:42 AEDT 2018_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The aim of this assignment is to enable you to **apply** different machine learning algorithms implemented in the Python [scikit-learn](http://scikit-learn.org/stable/index.html) machine learning library on a variety of datasets and answer questions based on your **analysis** and **interpretation** of the empirical\n",
    "results, using your knowledge of machine learning.\n",
    "\n",
    "After completing this assignment you will be able to:\n",
    "\n",
    "- set up replicated $k$-fold cross-validation experiments to obtain average\n",
    "  performance measures of algorithms on datasets\n",
    "- compare the performance of different algorithms against a base-line\n",
    "  and each other\n",
    "- aggregate comparative performance scores for algorithms over a range\n",
    "  of different datasets\n",
    "- propose properties of algorithms and their parameters, or datasets, which\n",
    "  may lead to performance differences being observed\n",
    "- suggest reasons for actual observed performance differences in terms of\n",
    "  properties of algorithms, parameter settings or datasets.\n",
    "- apply methods for data transformations and parameter search\n",
    "  and evaluate their effects on the performance of algorithms\n",
    "\n",
    "There are a total of *20 marks* available.\n",
    "Each is worth *0.5 course mark*, i.e., assignment marks will be scaled\n",
    "to a **course mark out of 10** to contribute to the course total.\n",
    "\n",
    "Deadline: 23:59:59, Monday April  2, 2018.\n",
    "\n",
    "Submission will be via the CSE *give* system (see below).\n",
    "\n",
    "Late penalties: one mark will be deducted from the total for each day late, up to a total of five days. If six or more days late, no marks will be given.\n",
    "\n",
    "Recall the guidance regarding plagiarism in the course introduction: this applies to this assignment and if evidence of plagiarism is detected it may result in penalties ranging from loss of marks to suspension.\n",
    "\n",
    "### Format of the questions\n",
    "\n",
    "There are 4 questions in this assignment. Each question has two parts: the Python code which must be run to generate the output results on the given datasets, and the responses you give in the file [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt) on your analysis and interpretation of the results  produced by running these learning algorithms for the question. Marks are given for both parts: submitting correct output from the code, and giving correct responses. For each question, you will need to save the output results from running the code to a separate plain text file. There will also be a plain text file containing the questions which you will need to edit to specify your answers. These files will form your submission.\n",
    "\n",
    "In summary, your submission will comprise a total of 5 (five) files which should be named as follows:\n",
    "```\n",
    "q1.out\n",
    "q2.out\n",
    "q3.out\n",
    "q4.out\n",
    "answers.txt\n",
    "```\n",
    "Please note: files in any format other than plain text **cannot be accepted**.\n",
    "\n",
    "Submit your files using ```give```. On a CSE Linux machine, type the following on the command-line:\n",
    "```\n",
    "$ give cs9417 ass1 q1.out q2.out q3.out q4.out answers.txt\n",
    "```\n",
    "\n",
    "Alternatively, you can submit using the web-based interface to ```give```.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "You can download the datasets required for the assignment [here](http://www.cse.unsw.edu.au/~mike/comp9417/ass1/datasets.zip).\n",
    "Note: you will need to ensure the dataset files are in the same directory from which you are started the notebook.\n",
    "\n",
    "**Please Note**: to load datasets from '.arff' formatted files, you will need to have installed the ```liac-arff``` package. You can do this using ```pip``` at the command-line, as follows:\n",
    "\n",
    "```\n",
    "$ pip install liac-arff\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 1\n",
    "\n",
    "For this question the objective is to run two different learning algorithms on a range of different sample sizes taken from the same training set to assess the effect of training sample size on error. You will use the nearest neighbour classifier and the decision tree classifier to generate two different sets of \"learning curves\" on 8 real-world datasets:\n",
    "\n",
    "```\n",
    "anneal.arff\n",
    "audiology.arff\n",
    "autos.arff\n",
    "credit-a.arff\n",
    "hypothyroid.arff\n",
    "letter.arff\n",
    "microarray.arff\n",
    "vote.arff\n",
    "```\n",
    "\n",
    "\n",
    "### Running the classifiers  [2 marks]\n",
    "You will run the following code section, and save the results to a plain text file \"q1.out\". You will also need to write your own code to compute the error reduction for question 1(b).\n",
    "The output of the code section are two tables, which represent the percentage error of classification for the nearest neighbour and the decision tree algorithm respectively. The first column contains the result of the baseline classifier, which simply predicts the majority class. From the second column on, the results are obtained by running the nearest neighbour or decision tree algorithms on $10\\%$, $25\\%$, $50\\%$, $75\\%$, and $100\\%$ of the data. The standard deviation are shown in brackets, and where an asterisk is present, it indicates that the result is significantly different from the baseline.\n",
    "\n",
    "\n",
    "### Result interpretation  [6 marks]\n",
    "Answer these questions in the file called [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt). Your answers must be based on the results you saved in \"q1.out\". **_Please note_**: the goal of these questions is to attempt to **_explain why_** you think the results you obtained are as they are.\n",
    "\n",
    "**1(a). [2 marks]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt).\n",
    "\n",
    "**1(b). [4 marks]** For each algorithm over all of the datasets, find the average change in error when moving from the default prediction to learning from 10% of the training set as follows.\n",
    "Let the error on the base line be err<sub>0</sub> and the error on 10% of the training set be error<sub>10</sub>.\n",
    "For each algorithm, calculate the percentage reduction in error relative to the default on each dataset as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{err_0 - err_{10}}{err_{0}} \\times 100.\n",
    "\\end{equation*}\n",
    "\n",
    "Now repeat exactly the same process by comparing the two classifiers over all of the datasets, learning from $100\\%$ of the training set, compared to default. Organise your results by grouping them into a 2 by 2 table, something like this:\n",
    "\n",
    "|  | Mean error reduction relative to default |\n",
    "|---|---|---|\n",
    "| Algorithm | After 10% training | After 100% training |\n",
    "| Nearest Neighbour | Your result | Your result |\n",
    "| Decision Tree | Your result | Your result |\n",
    "\n",
    "The entries from this table should be inserted into the correct places in your file [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt).\n",
    "\n",
    "Once you have done this, complete the rest of the answers for question 1 in your file [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Nearest Neighbour Results                                                 \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |  Baseline  |       10%        |       25%        |       50%        |       75%        |       100%      \n",
      "anneal          |     23.83% | 20.31% (0.94%) * | 18.00% (1.33%) * | 11.14% (0.70%) * |  9.11% (0.37%) * |  7.44% (0.44%) *\n",
      "audiology       |     74.77% | 60.17% (2.17%) * | 42.00% (2.56%) * | 31.85% (2.13%) * | 29.62% (1.78%) * | 26.47% (1.81%) *\n",
      "autos           |     67.35% | 64.50% (1.50%) * | 61.40% (2.21%) * | 65.96% (2.02%)   | 52.92% (2.39%) * | 57.37% (0.95%) *\n",
      "credit-a        |     44.49% | 39.98% (1.05%) * | 41.35% (0.99%) * | 32.04% (1.50%) * | 34.63% (0.79%) * | 34.71% (0.73%) *\n",
      "hypothyroid     |      7.71% |  8.27% (0.52%) * |  7.33% (0.18%) * |  4.74% (0.14%) * |  5.01% (0.13%) * |  4.79% (0.10%) *\n",
      "letter          |     96.26% | 16.86% (0.35%) * |  9.61% (0.20%) * |  6.05% (0.08%) * |  4.71% (0.06%) * |  3.93% (0.07%) *\n",
      "microarray      |     50.20% | 59.47% (2.55%) * | 49.58% (2.36%)   | 42.45% (0.83%) * | 50.71% (0.95%)   | 50.88% (0.60%)  \n",
      "vote            |     38.63% |  6.45% (1.01%) * | 10.42% (1.16%) * |  8.26% (0.55%) * |  7.12% (0.19%) * |  7.91% (0.39%) *\n",
      "\n",
      "                                                   Decision Tree Results                                                   \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |  Baseline  |       10%        |       25%        |       50%        |       75%        |       100%      \n",
      "anneal          |     23.83% |  8.72% (1.52%) * |  3.75% (0.70%) * |  1.36% (0.44%) * |  1.41% (0.42%) * |  0.69% (0.30%) *\n",
      "audiology       |     74.77% | 62.50% (3.27%) * | 46.33% (4.13%) * | 29.23% (1.86%) * | 22.36% (1.83%) * | 22.08% (1.98%) *\n",
      "autos           |     67.35% | 68.50% (7.43%)   | 46.80% (3.69%) * | 33.49% (2.41%) * | 30.17% (3.39%) * | 21.15% (3.40%) *\n",
      "credit-a        |     44.49% | 20.05% (2.52%) * | 13.20% (1.46%) * | 19.70% (1.86%) * | 19.48% (1.55%) * | 18.99% (1.15%) *\n",
      "hypothyroid     |      7.71% |  2.84% (0.54%) * |  1.55% (0.23%) * |  0.67% (0.04%) * |  0.80% (0.12%) * |  0.60% (0.07%) *\n",
      "letter          |     96.26% | 28.79% (0.41%) * | 21.52% (0.32%) * | 16.43% (0.27%) * | 13.33% (0.14%) * | 11.77% (0.15%) *\n",
      "microarray      |     50.20% | 48.77% (3.88%)   | 52.57% (3.75%)   | 50.31% (1.82%)   | 46.52% (2.21%) * | 49.15% (1.79%)  \n",
      "vote            |     38.63% | 12.75% (3.03%) * |  5.82% (1.64%) * |  6.95% (0.98%) * |  3.29% (0.58%) * |  5.74% (0.54%) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code for question 1\n",
    "\n",
    "import arff\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "seeds = [2, 3, 5, 7, 11, 13, 17, 23, 29, 31, 37]\n",
    "score_list = []\n",
    "\n",
    "for fname in [\"anneal.arff\", \"audiology.arff\", \"autos.arff\", \"credit-a.arff\", \\\n",
    "              \"hypothyroid.arff\", \"letter.arff\", \"microarray.arff\", \"vote.arff\"]:\n",
    "    dataset = arff.load(open(fname), 'r')\n",
    "    data = np.array(dataset['data'])\n",
    "\n",
    "    X = np.array(data)[:, :-1]\n",
    "    Y = np.array(data)[:, -1]\n",
    "\n",
    "    # turn unknown/none/? into a separate value\n",
    "    for i, j in product(range(len(data)), range(len(data[0]) - 1)):\n",
    "        if X[i, j] is None:\n",
    "            X[i, j] = len(dataset['attributes'][j][1])\n",
    "\n",
    "    # a hack to turn negative categories positive for autos.arff\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i] < 0:\n",
    "            Y[i] += 7\n",
    "\n",
    "    # identify and extract categorical/non-categorical features\n",
    "    categorical, non_categorical = [], []\n",
    "    for i in range(len(dataset['attributes']) - 1):\n",
    "        if isinstance(dataset['attributes'][i][1], str):\n",
    "            non_categorical.append(X[:, i])\n",
    "        else:\n",
    "            categorical.append(X[:, i])\n",
    "\n",
    "    categorical = np.array(categorical).T\n",
    "    non_categorical = np.array(non_categorical).T\n",
    "\n",
    "    if categorical.shape[0] == 0:\n",
    "        transformed_X = non_categorical\n",
    "    else:\n",
    "        # encode categorical features\n",
    "        encoder = OneHotEncoder(n_values = 'auto',\n",
    "                                categorical_features = 'all',\n",
    "                                dtype = np.int32,\n",
    "                                sparse = False,\n",
    "                                handle_unknown = 'error')\n",
    "        encoder.fit(categorical)\n",
    "        categorical = encoder.transform(categorical)\n",
    "        if non_categorical.shape[0] == 0:\n",
    "            transformed_X = categorical\n",
    "        else:\n",
    "            transformed_X = np.concatenate((categorical, non_categorical), axis = 1)\n",
    "\n",
    "    # concatenate the feature array and the labels for resampling purpose\n",
    "    Y = np.array([Y], dtype = np.int)\n",
    "    input_data = np.concatenate((transformed_X, Y.T), axis = 1)\n",
    "\n",
    "    # build the models\n",
    "    models = [DummyClassifier(strategy = 'most_frequent')] \\\n",
    "              + [KNeighborsClassifier(n_neighbors = 1, algorithm = \"brute\")] * 5 \\\n",
    "              + [DecisionTreeClassifier()] * 5\n",
    "\n",
    "    # resample and run cross validation\n",
    "    portion = [1.0, 0.1, 0.25, 0.5, 0.75, 1.0, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "    sample, scores = [None] * 11, [None] * 11\n",
    "    for i in range(11):\n",
    "        sample[i] = resample(input_data,\n",
    "                             replace = False,\n",
    "                             n_samples = int(portion[i] * input_data.shape[0]),\n",
    "                             random_state = seeds[i])\n",
    "        score = [None] * 10\n",
    "        for j in range(10):\n",
    "            score[j] = np.mean(cross_val_score(models[i],\n",
    "                                               sample[i][:, :-1],\n",
    "                                               sample[i][:, -1].astype(np.int),\n",
    "                                               scoring = 'accuracy',\n",
    "                                               cv = KFold(10, True, seeds[j])))\n",
    "        scores[i] = score\n",
    "\n",
    "    score_list.append((fname[:-5], 1 - np.array(scores)))\n",
    "\n",
    "# print the results\n",
    "header = [\"{:^123}\".format(\"Nearest Neighbour Results\") + '\\n' + '-' * 123  + '\\n' + \\\n",
    "          \"{:^15} | {:^10} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16}\" \\\n",
    "          .format(\"Dataset\", \"Baseline\", \"10%\", \"25%\", \"50%\", \"75%\", \"100%\"),\n",
    "          \"{:^123}\".format(\"Decision Tree Results\") + '\\n' + '-' * 123  + '\\n' + \\\n",
    "          \"{:^15} | {:^10} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16}\" \\\n",
    "          .format(\"Dataset\", \"Baseline\", \"10%\", \"25%\", \"50%\", \"75%\", \"100%\")]\n",
    "offset = [1, 6]\n",
    "\n",
    "for k in range(2):\n",
    "    print(header[k])\n",
    "    for i in range(8):\n",
    "        scores = score_list[i][1]\n",
    "        p_value = [None] * 5\n",
    "        for j in range(5):\n",
    "            _, p_value[j] = ttest_ind(scores[0], scores[j + offset[k]], equal_var = False)\n",
    "\n",
    "        print(\"{:<15} | {:>10.2%}\".format(score_list[i][0], np.mean(scores[0])), end = '')\n",
    "        for j in range(5):\n",
    "            print(\" | {:>6.2%} ({:>5.2%}) {}\" .format(np.mean(scores[j + offset[k]]),\n",
    "                                                      np.std(scores[j + offset[k]]),\n",
    "                                                      '*' if p_value[j] < 0.05 else ' '), end = '')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Dealing with noisy data is a key issue in machine learning. Unfortunately, even algorithms that have noise-handling mechanisms built-in, like decision trees, can overfit noisy data, unless their \"overfitting avoidance\" or *regularization* parameters are set properly.\n",
    "\n",
    "The datasets you will be using have had various amounts of \"class noise\" added\n",
    "by randomly changing the actual class value to a different one for a\n",
    "specified percentage of the training data.\n",
    "Here we will specify three arbitrarily chosen levels of noise: low\n",
    "($20\\%$), medium ($50\\%$) and high ($80\\%$).\n",
    "The learning algorithm must try to \"see through\" this noise and learn\n",
    "the best model it can, which is then evaluated on test data *without*\n",
    "added noise to evaluate how well it has avoided fitting the noise.\n",
    "\n",
    "We will also let the algorithm do a limited search using cross-validation\n",
    "for the best *over-fitting avoidance* parameter settings on each training set.\n",
    "\n",
    "### Running the classifiers  [2 marks]\n",
    "You will run the following code section, and save the results to a plain text file \"q2.out\". \n",
    "\n",
    "The output of the code section is a table, which represents the percentage accuaracy of classification for the decision tree algorithm. The first column contains the result of the \"Default\" classifier, which is the decision tree algorithm with default parameter settings running on each of the datasets which have had $50\\%$ noise added. From the second column on, in each column the results are obtained by running the decision tree algorithm on $0\\%$, $20\\%$, $50\\%$ and $80\\%$ noise added to each of the datasets, and in the parentheses is shown the result of a [grid search](http://en.wikipedia.org/wiki/Hyperparameter_optimization) that has been applied to determine the best value for a basic parameter of the decision tree algorithm, namely [min_samples_leaf](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) i.e., the minimum number of examples that can be used to make a prediction in the tree, on that dataset. \n",
    "\n",
    "### Result interpretation  [3 marks]\n",
    "Answer these questions in the file called [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt). Your answers must be based on the results you saved in \"q2.out\".\n",
    "\n",
    "**2(a). [2 marks]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt).\n",
    "\n",
    "**2(b). [1 mark]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# code for question 2\n",
    "\n",
    "import arff, numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fixed random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def label_enc(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le\n",
    "\n",
    "def features_encoders(features,categorical_features='all'):\n",
    "    n_samples, n_features = features.shape\n",
    "    label_encoders = [preprocessing.LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        label_encoders[i].fit(feature_i)\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "        \n",
    "    enc = preprocessing.OneHotEncoder(categorical_features=categorical_features)\n",
    "    return enc.fit(X_int),label_encoders\n",
    "\n",
    "def feature_transform(features,label_encoders, one_hot_encoder):\n",
    "    \n",
    "    n_samples, n_features = features.shape\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "\n",
    "    return one_hot_encoder.transform(X_int).toarray()\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    dataset = arff.load(open(path, 'r'))\n",
    "    data = np.array(dataset['data'])\n",
    "    data = pd.DataFrame(data)\n",
    "    data = DataFrameImputer().fit_transform(data).values\n",
    "    attr = dataset['attributes']\n",
    "\n",
    "    # mask categorical features\n",
    "    masks = []\n",
    "    for i in range(len(attr)-1):\n",
    "        if attr[i][1] != 'REAL':\n",
    "            masks.append(i)\n",
    "    return data, masks\n",
    "\n",
    "def preprocess(data,masks, noise_ratio):\n",
    "    # split data\n",
    "    train_data, test_data = train_test_split(data,test_size=0.3,random_state=0)\n",
    "\n",
    "    # test data\n",
    "    test_features = test_data[:,0:test_data.shape[1]-1]\n",
    "    test_labels = test_data[:,test_data.shape[1]-1]\n",
    "\n",
    "    # training data\n",
    "    features = train_data[:,0:train_data.shape[1]-1]\n",
    "    labels = train_data[:,train_data.shape[1]-1]\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    # categorical features need to be encoded\n",
    "    if len(masks):\n",
    "        one_hot_enc, label_encs = features_encoders(data[:,0:data.shape[1]-1],masks)\n",
    "        test_features = feature_transform(test_features,label_encs,one_hot_enc)\n",
    "        features = feature_transform(features,label_encs,one_hot_enc)\n",
    "\n",
    "    le = label_enc(data[:,data.shape[1]-1])\n",
    "    labels = le.transform(train_data[:,train_data.shape[1]-1])\n",
    "    test_labels = le.transform(test_data[:,test_data.shape[1]-1])\n",
    "    \n",
    "    # add noise\n",
    "    np.random.seed(1234)\n",
    "    noise = np.random.randint(len(classes)-1, size=int(len(labels)*noise_ratio))+1\n",
    "    \n",
    "    noise = np.concatenate((noise,np.zeros(len(labels) - len(noise),dtype=np.int)))\n",
    "    labels = (labels + noise) % len(classes)\n",
    "\n",
    "    return features,labels,test_features,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Decision Tree Results                                                   \n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |     Default      |        0%        |       20%        |       50%        |       80%        |\n",
      "balance-scale   |  36.70% ( 2)     |  76.06% ( 2)     |  71.28% (12)     |  65.43% (27)     |  18.09% (27)     |\n",
      "\n",
      "primary-tumor   |  25.49% ( 2)     |  37.25% (12)     |  42.16% (12)     |  43.14% (12)     |  26.47% ( 7)     |\n",
      "\n",
      "glass           |  44.62% ( 2)     |  69.23% ( 7)     |  66.15% (22)     |  35.38% (17)     |  29.23% (17)     |\n",
      "\n",
      "heart-h         |  35.96% ( 2)     |  67.42% ( 7)     |  78.65% (22)     |  56.18% (17)     |  20.22% (27)     |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "paths = ['balance-scale','primary-tumor',\n",
    "         'glass','heart-h']\n",
    "noise = [0,0.2,0.5,0.8]\n",
    "\n",
    "scores = []\n",
    "params = []\n",
    "\n",
    "for path in paths:\n",
    "    score = []\n",
    "    param = []\n",
    "    path += '.arff'\n",
    "    data, masks = load_data(path)\n",
    "    \n",
    "    # training on data with %50 noise and default parameters\n",
    "    features, labels, test_features, test_labels = preprocess(data, masks, 0.5)\n",
    "    tree = DecisionTreeClassifier(random_state=0,min_samples_leaf=2, min_impurity_decrease=0)\n",
    "    tree.fit(features, labels)\n",
    "    tree_preds = tree.predict(test_features)\n",
    "    tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "    score.append(tree_performance)\n",
    "    param.append(tree.get_params()['min_samples_leaf'])\n",
    "    \n",
    "    # training on data with noise %0, %20, %50, %80\n",
    "    for noise_ratio in noise:\n",
    "        features, labels, test_features, test_labels = preprocess(data, masks, noise_ratio)\n",
    "        param_grid = {'min_samples_leaf': np.arange(2,30,5)}\n",
    "\n",
    "        grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid,cv=10,return_train_score=True)\n",
    "        grid_tree.fit(features, labels)\n",
    "\n",
    "        estimator = grid_tree.best_estimator_\n",
    "        tree_preds = grid_tree.predict(test_features)\n",
    "        tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "        score.append(tree_performance)\n",
    "        param.append(estimator.get_params()['min_samples_leaf'])\n",
    "\n",
    "    scores.append(score)\n",
    "    params.append(param)\n",
    "\n",
    "# print the results\n",
    "header = \"{:^123}\".format(\"Decision Tree Results\") + '\\n' + '-' * 123  + '\\n' + \\\n",
    "\"{:^15} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16} |\".format(\"Dataset\", \"Default\", \"0%\", \"20%\", \"50%\", \"80%\")\n",
    "\n",
    "\n",
    "# print result table\n",
    "print(header)\n",
    "for i in range(len(scores)):\n",
    "    #scores = score_list[i][1]\n",
    "    print(\"{:<16}\".format(paths[i]),end=\"\")\n",
    "    for j in range(len(params[i])):\n",
    "        print(\"|  {:>6.2%} ({:>2})     \" .format(scores[i][j],params[i][j]),end=\"\")\n",
    "    print('|\\n')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 3\n",
    "\n",
    "This question involves mining a data-set on California house prices from\n",
    "census data in the 1990s.\n",
    "We will be using linear regression to do this since the output is numeric.\n",
    "Since this problem involves using attribute or feature transformations we\n",
    "will need to do this using the [*numpy*](http://www.numpy.org/) Python library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### Running the regression  [1 mark]\n",
    "\n",
    "In this question, in the following code section you will be required to train a linear regression model using scikit learn to fit the dataset. Select \"median house value\" as the target variable Y and the rest of the features as X. Then perform a 10-fold cross validation of  linear regression on the dataset. Save the Intercept and Coefficients of the reuslting Linear regression model, as well as Root mean squared error of cross validation, into a plain text file called \"q3.out\". For this question the code to save the output to a file has been provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['median_house_value', 'REAL'],\n",
       "       ['median_income', 'REAL'],\n",
       "       ['housing_median_age', 'REAL'],\n",
       "       ['total_rooms', 'REAL'],\n",
       "       ['total_bedrooms', 'REAL'],\n",
       "       ['population', 'REAL'],\n",
       "       ['households', 'REAL'],\n",
       "       ['latitude', 'REAL'],\n",
       "       ['longitude', 'REAL']], dtype='<U18')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for question 3\n",
    "\n",
    "import arff,numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#--------------Show the attributes--------------\n",
    "\n",
    "dataset = arff.load(open('houses.arff',\"r\",encoding = \"ISO-8859-1\")) \n",
    "attributes = np.array(dataset['attributes'])\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:\n",
      "-3.59e+06\n",
      "Coefficients:\n",
      "4.02e+04 1.16e+03 -8.18e+00 1.13e+02 -3.85e+01 4.83e+01 -4.26e+04 -4.28e+04 \n",
      "RMSE:\n",
      "7.13e+04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------linear regression--------------\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "data = np.array(dataset['data'])\n",
    "houses_X = data[:,1:] #X vector\n",
    "houses_Y = data[:,0] #Y vector\n",
    "\n",
    "regr.fit(houses_X, houses_Y)\n",
    "intercept = regr.intercept_\n",
    "\n",
    "print('Intercept:\\n%.2e' % intercept,end='\\n')\n",
    "print('Coefficients:')\n",
    "for coef in regr.coef_:\n",
    "    print('%.2e' % coef,end=\" \")\n",
    "\n",
    "file = open('q3.out','a')\n",
    "file.write('Intercept:\\n%.2e\\n' % intercept)\n",
    "file.write('Coefficients:\\n')\n",
    "for coef in regr.coef_:\n",
    "    file.write('%.2e' % coef)\n",
    "    file.write(' ')\n",
    "file.write('\\n')\n",
    "\n",
    "#--------------10-fold cross validation--------------\n",
    "\n",
    "predicted = cross_val_predict(regr, houses_X, houses_Y, cv=10)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(houses_Y, predicted))\n",
    "\n",
    "print ('\\nRMSE:\\n%.2e\\n' % RMSE)\n",
    "file.write('RMSE:\\n%.2e\\n' % RMSE)\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data transformation (feature construction) \n",
    "\n",
    "Now set up a log transform to the class variable \"median house value\". Then train a new linear regression model to fit the transformed dataset. Save (append) the Intercept and Coefficients into your \"q3.out\" file.\n",
    "\n",
    "Experiment with at most two more sets of transformations to the variables(including squares, cubes, and logs of ratios), run linear regression on the transformed data.\n",
    "\n",
    "### Result interpretation  [3 marks]\n",
    "\n",
    "**3(a). [2 marks]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt).\n",
    "\n",
    "**3(b). [1 mark]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:\n",
      "-1.18e+01\n",
      "Coefficients:\n",
      "1.78e-01 3.26e-03 -3.19e-05 4.80e-04 -1.72e-04 2.49e-04 -2.80e-01 -2.76e-01 \n",
      "RMSE:\n",
      "3.50e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------linear regression on transformed dataset--------------\n",
    "\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "houses_X = data[:,1:] #X vector\n",
    "houses_Y = data[:,0] #Y vector\n",
    "#-----------log tranformation------------------\n",
    "\n",
    "logMedianHousePrice = np.log(houses_Y)\n",
    "regr.fit(houses_X, logMedianHousePrice)\n",
    "intercept = regr.intercept_\n",
    "\n",
    "print('Intercept:\\n%.2e' % intercept)\n",
    "print('Coefficients:')\n",
    "for coef in regr.coef_:\n",
    "    print('%.2e' % coef,end=\" \")\n",
    "\n",
    "file = open('q3.out','a')\n",
    "file.write('Intercept:\\n%.2e\\n' % intercept)\n",
    "file.write('Coefficients:\\n')\n",
    "for coef in regr.coef_:\n",
    "    file.write('%.2e' % coef)\n",
    "    file.write(' ')\n",
    "file.write('\\n')\n",
    "\n",
    "#--------------10-fold cross validation--------------\n",
    "\n",
    "predicted = cross_val_predict(regr, houses_X, logMedianHousePrice, cv=10)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(logMedianHousePrice, predicted))\n",
    "\n",
    "print ('\\nRMSE:\\n%.2e\\n'% RMSE)\n",
    "file.write('RMSE:\\n%.2e\\n' % RMSE)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 4\n",
    "\n",
    "This question involves mining text data, for which machine learning algorithms typically use a transformation into a dataset of \"word counts\". In the original dataset each text example is a string of words with a class label, and the sklearn transform converts this to a vector of word counts.\n",
    "\n",
    "The dataset contains \"snippets\", short sequences of words taken from Google searches, each of which has been labelled with one of 8 classes, referred to as \"sections\", such as business, sports, etc. The dataset is provided already split into a training set of $10,060$ snippets and a test set of $2,280$ snippets (for convenience, the combined dataset is also provided as a separate file). \n",
    "\n",
    "Using a vector representation for text data means that we can use many of the standard classifier learning methods. However, such datasets are typically  highly â€œsparseâ€, in the sense that for any example (i.e., piece of text) nearly all of its feature values are zero. To tackle this problem, we typically apply methods of feature selection (or dimensionality reduction). In this question you will investigate the effect of using the [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) method to select the $K$ best features (words or tokens in this case) that appear to help classification accuracy.\n",
    "\n",
    "### Running the classifier  [1 mark]\n",
    "You will run the following code section, and save the results to a plain text file \"q4.out\". \n",
    "\n",
    "The output of the code section is 5 lines of output, each of which represents the percentage accuaracy of classification on training and test set for different amounts of feature selection.\n",
    "The first such line representst the \"default\", i.e., using all features. The remaining 4 lines show the effect of learning and predicting on text data where only the top $K$ features are being used.\n",
    "\n",
    "\n",
    "### Result interpretation  [2 marks]\n",
    "Answer this question in the file called [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt). Your answers must be based on the results you saved in \"q4.out\".\n",
    "\n",
    "**4. [2 marks]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/18s1/ass1/answers.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test accuracy using all features:  0.979324055666004 0.8052631578947368\n"
     ]
    }
   ],
   "source": [
    "# code for question 4\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "df_trte = pd.read_csv('snippets_all.csv')\n",
    "df_tr = pd.read_csv('snippets_train.csv')\n",
    "df_te = pd.read_csv('snippets_test.csv')\n",
    "\n",
    "# set up the vocabulary (the global set of \"words\" or tokens) for training and test datasets\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df_trte.snippet)\n",
    "\n",
    "# apply this vocabulary to transform the text snippets to vectors of word counts\n",
    "X_train = vectorizer.transform(df_tr.snippet)\n",
    "X_test = vectorizer.transform(df_te.snippet)\n",
    "y_train = df_tr.section\n",
    "y_test = df_te.section\n",
    "\n",
    "# Debugging\n",
    "# print(\"X train: \", X_train.shape)\n",
    "# print(\"X test: \", X_test.shape)\n",
    "# print(\"Y train: \", y_train.shape)\n",
    "# print(\"Y test: \", y_test.shape)\n",
    "\n",
    "# learn a Naive Bayes classifier on the training set\n",
    "clf = MultinomialNB(alpha=0.5)\n",
    "MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf.fit(X_train, y_train)\n",
    "pred_train = clf.predict(X_train)\n",
    "score_train = metrics.accuracy_score(y_train, pred_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "score_test = metrics.accuracy_score(y_test, pred_test)\n",
    "print(\"Train/test accuracy using all features: \", score_train, score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test accuracy for top 10K features 0.9717693836978131 0.8052631578947368\n",
      "Train/test accuracy for top 1K features 0.9538767395626243 0.6942982456140351\n",
      "Train/test accuracy for top 100 features 0.7187872763419483 0.4631578947368421\n",
      "Train/test accuracy for top 10 features 0.4092445328031809 0.18903508771929825\n"
     ]
    }
   ],
   "source": [
    "# Use Chi^2 to select top 10000 features\n",
    "ch2_10000 = SelectKBest(chi2, k=10000)\n",
    "ch2_10000.fit(X_train, y_train)\n",
    "# Project training data onto top 10000 selected features\n",
    "X_train_kbest_10000 = ch2_10000.transform(X_train)\n",
    "# Train NB Classifier using top 10 selected features\n",
    "clf_kbest_10000 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_10000.fit(X_train_kbest_10000,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_10000 = clf_kbest_10000.predict(X_train_kbest_10000)\n",
    "score_train_kbest_10000 = metrics.accuracy_score(y_train,pred_train_kbest_10000)\n",
    "# Project test data onto top 10000 selected features\n",
    "X_test_kbest_10000 = ch2_10000.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_10000 = clf_kbest_10000.predict(X_test_kbest_10000)\n",
    "score_test_kbest_10000 = metrics.accuracy_score(y_test,pred_test_kbest_10000)\n",
    "print(\"Train/test accuracy for top 10K features\", score_train_kbest_10000, score_test_kbest_10000)\n",
    "\n",
    "# Use Chi^2 to select top 1000 features\n",
    "ch2_1000 = SelectKBest(chi2, k=1000)\n",
    "ch2_1000.fit(X_train, y_train)\n",
    "# Project training data onto top 1000 selected features\n",
    "X_train_kbest_1000 = ch2_1000.transform(X_train)\n",
    "# Train NB Classifier using top 1000 selected features\n",
    "clf_kbest_1000 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_1000.fit(X_train_kbest_1000,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_1000 = clf_kbest_1000.predict(X_train_kbest_1000)\n",
    "score_train_kbest_1000 = metrics.accuracy_score(y_train,pred_train_kbest_1000)\n",
    "# Project test data onto top 1000 selected features\n",
    "X_test_kbest_1000 = ch2_1000.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_1000 = clf_kbest_1000.predict(X_test_kbest_1000)\n",
    "score_test_kbest_1000 = metrics.accuracy_score(y_test,pred_test_kbest_1000)\n",
    "print(\"Train/test accuracy for top 1K features\", score_train_kbest_1000, score_test_kbest_1000)\n",
    "\n",
    "# Use Chi^2 to select top 100 features\n",
    "ch2_100 = SelectKBest(chi2, k=100)\n",
    "ch2_100.fit(X_train, y_train)\n",
    "# Project training data onto top 100 selected features\n",
    "X_train_kbest_100 = ch2_100.transform(X_train)\n",
    "# Train NB Classifier using top 100 selected features\n",
    "clf_kbest_100 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_100.fit(X_train_kbest_100,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_100 = clf_kbest_100.predict(X_train_kbest_100)\n",
    "score_train_kbest_100 = metrics.accuracy_score(y_train,pred_train_kbest_100)\n",
    "# Project test data onto top 100 selected features\n",
    "X_test_kbest_100 = ch2_100.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_100 = clf_kbest_100.predict(X_test_kbest_100)\n",
    "score_test_kbest_100 = metrics.accuracy_score(y_test,pred_test_kbest_100)\n",
    "print(\"Train/test accuracy for top 100 features\", score_train_kbest_100, score_test_kbest_100)\n",
    "\n",
    "# Use Chi^2 to select top 10 features\n",
    "ch2_10 = SelectKBest(chi2, k=10)\n",
    "ch2_10.fit(X_train, y_train)\n",
    "# Project training data onto top 10 selected features\n",
    "X_train_kbest_10 = ch2_10.transform(X_train)\n",
    "# Train NB Classifier using top 10 selected features\n",
    "clf_kbest_10 = MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
    "clf_kbest_10.fit(X_train_kbest_10,y_train)\n",
    "# Predictive accuracy on training set\n",
    "pred_train_kbest_10 = clf_kbest_10.predict(X_train_kbest_10)\n",
    "score_train_kbest_10 = metrics.accuracy_score(y_train,pred_train_kbest_10)\n",
    "# Project test data onto top 10 selected features\n",
    "X_test_kbest_10 = ch2_10.transform(X_test)\n",
    "# Predictive accuracy on test set\n",
    "pred_test_kbest_10 = clf_kbest_10.predict(X_test_kbest_10)\n",
    "score_test_kbest_10 = metrics.accuracy_score(y_test,pred_test_kbest_10)\n",
    "print(\"Train/test accuracy for top 10 features\", score_train_kbest_10, score_test_kbest_10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
